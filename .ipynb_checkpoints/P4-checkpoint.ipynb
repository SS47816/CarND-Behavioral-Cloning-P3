{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behavioral Cloning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run only once, to solve the conflict with ROS\n",
    "import sys\n",
    "sys.path.remove('/opt/ros/kinetic/lib/python2.7/dist-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "samples = []\n",
    "# with open('data/driving_log.csv') as csvfile:\n",
    "with open('data/driving_log.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for line in reader:\n",
    "        samples.append(line)  \n",
    "        \n",
    "from sklearn.model_selection import train_test_split\n",
    "train_samples, validation_samples = train_test_split(samples, \n",
    "                                                     test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_cam(batch_sample):\n",
    "    correction = 0.2\n",
    "    \n",
    "    multi_cam_images, multi_cam_angles = [], []\n",
    "    for i in range(0, 3):\n",
    "        name = 'data/IMG/' + batch_sample[i].split('/')[-1]\n",
    "        image = plt.imread(name)\n",
    "        # if using the left/right/center camera\n",
    "        if(i == 1): \n",
    "            angle = float(batch_sample[3]) + correction\n",
    "        elif(i == 2): \n",
    "            angle = float(batch_sample[3]) - correction\n",
    "        else: \n",
    "            angle = float(batch_sample[3])\n",
    "        multi_cam_images.append(image)\n",
    "        multi_cam_angles.append(angle)\n",
    "    return multi_cam_images, multi_cam_angles\n",
    "\n",
    "\n",
    "def flip_img(images, angles):\n",
    "    fliped_images, fliped_angles = [], []\n",
    "    for image in images:\n",
    "        fliped_images.append(image)\n",
    "        fliped_images.append(np.fliplr(image))\n",
    "    for angle in angles:\n",
    "        fliped_angles.append(angle)\n",
    "        fliped_angles.append(angle * (-1))\n",
    "    return fliped_images, fliped_angles\n",
    "\n",
    "\n",
    "def generator(samples, batch_size=32):\n",
    "    num_samples = len(samples)\n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "        shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "            images, angles = [], []\n",
    "            for batch_sample in batch_samples:\n",
    "                \n",
    "                # Using all three cameras, output 3*1 array of images and angles\n",
    "                multi_cam_images, multi_cam_angles = multi_cam(batch_sample)\n",
    "                # flip all the camera images, output 6*1 array of images and angles\n",
    "                fliped_images, fliped_angles = flip_img(multi_cam_images, multi_cam_angles)\n",
    "                \n",
    "                output_images = fliped_images\n",
    "                output_angles = fliped_angles\n",
    "                images.extend(output_images)\n",
    "                angles.extend(output_angles)\n",
    "                \n",
    "                #name = 'data/IMG/'+batch_sample[0].split('/')[-1]\n",
    "                #center_image = plt.imread(name)\n",
    "                #center_angle = float(batch_sample[3])\n",
    "                #images.append(center_image)\n",
    "                #angles.append(center_angle)\n",
    "                \n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(angles)\n",
    "            yield shuffle(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nvidia End-to-End CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Cropping2D, Lambda, Conv2D, Flatten, Dense\n",
    "\n",
    "# Set our batch size\n",
    "batch_size = 32\n",
    "\n",
    "# compile and train the model using the generator function\n",
    "train_generator = generator(train_samples, batch_size=batch_size)\n",
    "validation_generator = generator(validation_samples, batch_size=batch_size)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Lambda(lambda x: x / 255.0 - 0.5, input_shape=(160,320,3)))\n",
    "model.add(Cropping2D(cropping=((70,25), (0,0))))\n",
    "model.add(Conv2D(24, (5, 5), activation=\"relu\", strides=(2, 2)))\n",
    "model.add(Conv2D(36, (5, 5), activation=\"relu\", strides=(2, 2)))\n",
    "model.add(Conv2D(48, (5, 5), activation=\"relu\", strides=(2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation=\"relu\"))\n",
    "model.add(Conv2D(64, (3, 3), activation=\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100))\n",
    "model.add(Dense(50))\n",
    "model.add(Dense(10))\n",
    "model.add(Dense(1))\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    model.fit_generator(train_generator, \n",
    "                        steps_per_epoch=np.ceil(len(train_samples)/batch_size), \n",
    "                        validation_data=validation_generator, \n",
    "                        validation_steps=np.ceil(len(validation_samples)/batch_size), \n",
    "                        epochs=5, verbose=1)\n",
    "\n",
    "model.save('model.h5')\n",
    "print(\"Model Saved\")\n",
    "# exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Cropping2D, Lambda, Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Set our batch size\n",
    "batch_size = 32\n",
    "\n",
    "# compile and train the model using the generator function\n",
    "train_generator = generator(train_samples, batch_size=batch_size)\n",
    "validation_generator = generator(validation_samples, batch_size=batch_size)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Lambda(lambda x: x / 255.0 - 0.5, input_shape=(160,320,3)))\n",
    "model.add(Cropping2D(cropping=((70,25), (0,0))))\n",
    "model.add(Conv2D(6, (5, 5), activation=\"relu\", strides=(1, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(16, (5, 5), activation=\"relu\", strides=(1, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(120))\n",
    "model.add(Dense(84))\n",
    "model.add(Dense(10))\n",
    "model.add(Dense(1))\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    model.fit_generator(train_generator, \n",
    "                        steps_per_epoch=np.ceil(len(train_samples)/batch_size), \n",
    "                        validation_data=validation_generator, \n",
    "                        validation_steps=np.ceil(len(validation_samples)/batch_size), \n",
    "                        epochs=5, verbose=1)\n",
    "\n",
    "model.save('lenet_model.h5')\n",
    "print(\"Model Saved\")\n",
    "# exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Behavioral Cloning** \n",
    "\n",
    "## Writeup Template\n",
    "\n",
    "### You can use this file as a template for your writeup if you want to submit it as a markdown file, but feel free to use some other method and submit a pdf if you prefer.\n",
    "\n",
    "---\n",
    "\n",
    "**Behavioral Cloning Project**\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "* Use the simulator to collect data of good driving behavior\n",
    "* Build, a convolution neural network in Keras that predicts steering angles from images\n",
    "* Train and validate the model with a training and validation set\n",
    "* Test that the model successfully drives around track one without leaving the road\n",
    "* Summarize the results with a written report\n",
    "\n",
    "\n",
    "[//]: # (Image References)\n",
    "\n",
    "[image1]: ./examples/issue.jpg \"Issue with the LeNet Model\"\n",
    "[image2]: ./examples/nvidia-cnn.png \"Model Visualization\"\n",
    "[image3]: ./examples/anti-clockwise.jpg \"Anti-clockwise Driving\"\n",
    "[image4]: ./examples/clockwise.jpg \"Clockwise Driving\"\n",
    "[image5]: ./examples/recover_left.jpg \"Recovery from Left\"\n",
    "[image6]: ./examples/recover_right.jpg \"Recovery from Right\"\n",
    "[image7]: ./examples/unfliped.jpg \"Normal Image\"\n",
    "[image8]: ./examples/fliped1.jpg \"Flipped Image\"\n",
    "\n",
    "## Rubric Points\n",
    "### Here I will consider the [rubric points](https://review.udacity.com/#!/rubrics/432/view) individually and describe how I addressed each point in my implementation.  \n",
    "\n",
    "---\n",
    "### Files Submitted & Code Quality\n",
    "\n",
    "#### 1. Submission includes all required files and can be used to run the simulator in autonomous mode\n",
    "\n",
    "My project includes the following files:\n",
    "* model.py containing the script to create and train the model\n",
    "* drive.py for driving the car in autonomous mode\n",
    "* model.h5 containing a trained convolution neural network \n",
    "* writeup_report.md or writeup_report.pdf summarizing the results\n",
    "\n",
    "#### 2. Submission includes functional code\n",
    "Using the Udacity provided simulator and my drive.py file, the car can be driven autonomously around the track by executing \n",
    "```sh\n",
    "python drive.py model.h5\n",
    "```\n",
    "\n",
    "#### 3. Submission code is usable and readable\n",
    "\n",
    "The model.py file contains the code for training and saving the convolution neural network. The file shows the pipeline I used for training and validating the model, and it contains comments to explain how the code works.\n",
    "\n",
    "### Model Architecture and Training Strategy\n",
    "\n",
    "#### 1. An appropriate model architecture has been employed\n",
    "\n",
    "In the first place, I used a modifed LeNet model to try driving the car autonomously. At most of the time, it can keep the car at the center of the road beautifully. However, at the intersection of the main road and the branch, the network tends to drive the car into that unconstructed part of the road (on the right):\n",
    "\n",
    "![alt text][image1]\n",
    "\n",
    "Then, I tried to record driving on the track both anti-clockwise and clockwise, hoping that it can generalize the model. The result showed that it really worked, through the car will pop onto the ledges on the right hand side of the road during that turn. \n",
    "\n",
    "To further improve the model, I decided to adopt a new architecture, which is the one nvidia published in their [paper](https://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf) and achieved very impressive results. \n",
    "\n",
    "The model includes RELU layers to introduce nonlinearity (code line 97-101), and the data is normalized in the model using a Keras lambda layer (code line 95), and corpped to a shape of 65x320x3 (code line 96).\n",
    "\n",
    "#### 2. Final Model Architecture\n",
    "\n",
    "Here is a visualization of the final model architecture (model.py lines 94-106)\n",
    "\n",
    "![alt text][image2]\n",
    "\n",
    "#### 3. Model parameter tuning\n",
    "\n",
    "The model used an adam optimizer, so the learning rate was not tuned manually (model.py line 109).\n",
    "\n",
    "#### 4. Appropriate training data\n",
    "\n",
    "To capture good driving behavior, I first recorded two laps on track one, driving anti-clockwise. Here is an example image of center lane driving:\n",
    "\n",
    "![alt text][image3]\n",
    "\n",
    "I then recorded another two laps on the same track, clockwise: \n",
    "\n",
    "![alt text][image4]\n",
    "\n",
    "During each lap, I tried to maintain the vehicle on the center of the road. Sometimes(Due to my bad driving skill), the vehicle is a bit off, and I use these moments to train the vehicle to recover from the left side and right sides of the road back to center. These images show what a recovery looks:\n",
    "\n",
    "![alt text][image5]\n",
    "![alt text][image6]\n",
    "\n",
    "\n",
    "To augment the data sat, I also flipped images and angles thinking that this would prevent the predictions biased towards one direction. For example, here is an image that has then been flipped:\n",
    "\n",
    "![alt text][image7]\n",
    "![alt text][image8]\n",
    "\n",
    "After the collection process, I had 27600 number of sets of data(including images from the center, left and right cameras, and their fliped copy). I randomly shuffled the data set and put 20% of the data into a validation set. \n",
    "\n",
    "I used this training data for training the model. The validation set helped determine if the model was over or under fitting. The ideal number of epochs was 5 as evidenced by the changes in loss after each epoch. I used an adam optimizer so that manually training the learning rate wasn't necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3] *",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
